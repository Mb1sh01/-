---
title: "Упражнение 8"
author: "Блинов Илья"
date: "25 04 2021"
output: html_document
---

Необходимо построить две модели для прогноза на основе дерева решений:
 - *для непрерывной зависимой переменной;* 
 - *для категориальной зависимой переменной.*
 
Данные и переменные указаны в таблице с вариантами.

Ядро генератора случайных чисел – номер варианта.

Задания Для каждой модели:

1. Указать настроечные параметры метода из своего варианта (например: количество узлов, количество предикторов, скорость обучения).

2. Подогнать модель на обучающей выборке (50% наблюдений). Рассчитать MSE на тестовой выборке.

3. Перестроить модель с помощью метода, указанного в варианте.

4. Сделать прогноз по модели с подобранными в п.3 параметрами на тестовой выборке, оценить его точность и построить график «прогноз-реализация».

Как сдавать: прислать на почту преподавателя ссылки: * на html-отчёт с видимыми блоками кода (блоки кода с параметром echo = T), размещённый на rpubs.com.
* на код, генерирующий отчёт, в репозитории на github.com. В текст отчёта включить постановку задачи и ответы на вопросы задания.

## Вариант 4

 - Метод подгонки моделей: бустинг

 - Данные: *Boston{MASS}*

```{r setup, include=FALSE}
library('tree')              # деревья tree()
library('MASS')              # набор данных Carseats
library('GGally')            # матричный график разброса 
#library('MASS')              # набор данных Boston
library('randomForest')      # случайный лес 
library('gbm')               # бустинг gbm()
library('class')
data(Boston)

# Ядро генератора случайных чисел
my.seed <- 4

knitr::opts_chunk$set(echo = TRUE)
```


```{r}
str(Boston)
head(Boston)
```

# Модель 1 (для непрерывной зависимой переменной Boston)

```{r}

# ?Boston
head(Boston)
```

```{r}
# матричные графики разброса переменных
p <- ggpairs(Boston[, c(14, 1:4)])
suppressMessages(print(p))

p <- ggpairs(Boston[, c(14, 5:8)])
suppressMessages(print(p))
p <- ggpairs(Boston[, c(14, 9:12)])
suppressMessages(print(p))
p <- ggpairs(Boston[, c(14, 12:13)])
suppressMessages(print(p))

```

```{r}
# Обучающая выборка
set.seed(my.seed)
# Обучающая выборка - 50%
train <- sample(1:nrow(Boston), nrow(Boston)/2)
```

Построим дерево регрессии для зависимой переменной Boston.

```{r}
# Обучаем модель
tree.medv <- tree(medv ~ ., Boston, subset = train)
summary(tree.medv)
```

```{r}
# Визуализация
plot(tree.medv)
text(tree.medv, pretty = 0)
                   
```

```{r}
# Прогноз по модели 
yhat <- predict(tree.medv, newdata = Boston[-train, ])
medv.test <- Boston[-train, "medv"]

# MSE на тестовой выборке
mse.test <- mean((yhat - medv.test)^2)
names(mse.test)[length(mse.test)] <- 'medv.regr.tree.all'
mse.test

# Точность прогноза на тестовой выборке
acc.test <- sum(abs(yhat-medv.test))/sum(medv.test)
names(acc.test)[length(acc.test)] <- 'medv.regr.tree.all'
acc.test
```

#Бустинг (модель 1)

Проведем бустинг с целью улучшения модели

```{r}
set.seed(my.seed)
boost.medv <- gbm(medv ~ ., data = Boston[train, ], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4)
# График и таблица относительной важности переменных
summary(boost.medv)
```

```{r}
# прогноз
yhat.boost <- predict(boost.medv, newdata = Boston[-train, ], n.trees = 5000)

# MSE на тестовой
mse.test <- c(mse.test, mean((yhat.boost - medv.test)^2))
names(mse.test)[length(mse.test)] <- 'medv.boost.opt'
mse.test

# Точность прогноза на тестовой выборке
acc.test <- c(acc.test, sum(abs(yhat.boost-medv.test))/sum(medv.test))
names(acc.test)[length(acc.test)] <- 'medv.regr.tree'
acc.test
```

```{r}
# Меняем значение гиперпараметра (lambda) на 0.1 -- аргумент shrinkage
boost.medv <- gbm(medv ~ ., data = Boston[train, ], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4, 
                    shrinkage = 0.1, verbose = F)

# Прогноз
yhat.boost <- predict(boost.medv, newdata = Boston[-train, ], n.trees = 5000)

# MSE а тестовой
mse.test <- c(mse.test, mean((yhat.boost - medv.test)^2))
names(mse.test)[length(mse.test)] <- 'medv.boost.0.1'
mse.test

# Точность прогноза на тестовой выборке
acc.test <- c(acc.test, sum(abs(yhat.boost-medv.test))/sum(medv.test))
names(acc.test)[length(acc.test)] <- 'medv.regr.tree.0.1'
acc.test
```

```{r}
# График "прогноз - реализация"
plot(yhat.boost, medv.test)
# линия идеального прогноза
abline(0, 1) 
```

Судя по результатам изменение lambda на 0.1 немного уменьшило ошибку прогноза. MSE модели (с бустингом) без указания lambda на тестовой выборке равна 16.31184, точность прогноза составила 0.1.

```{r}
# прогноз
yhat.boost <- predict(boost.medv, newdata = Boston[-train, ], n.trees = 5000)

# MSE на тестовой
mse.test <- c(mse.test, mean((yhat.boost - medv.test)^2))
names(mse.test)[length(mse.test)] <- 'medv.boost.opt'
mse.test
```


# Модель 2 (для категориальной зависимой переменной high.medv) 
Добавим переменную high.medv – “высокая стоимость домов” со значениями:

1, если medv >= 25
0, если medv < 25

```{r}
# новая переменная
high.medv <- ifelse(Boston$medv >= 25, 1, 0)
high.medv <- factor(high.medv, labels = c('yes', 'no'))
Boston$high.medv <- high.medv 
# матричные графики разброса переменных
p <- ggpairs(Boston[, c(15, 1:5)], aes(color = high.medv))
suppressMessages(print(p))

p <- ggpairs(Boston[, c(15, 6:10)], aes(color = high.medv))
suppressMessages(print(p))
p <- ggpairs(Boston[, c(15, 11:14)], aes(color = high.medv))
suppressMessages(print(p))
```


```{r}
# модель бинарного  дерева без переменных Boston и name
tree.medv <- tree(high.medv ~ .-medv, Boston)
summary(tree.medv)
```

```{r}
# график результата:
# ветви
plot(tree.medv)
# добавим подписи
text(tree.medv, pretty = 0)

```

Теперь построим дерево на обучающей выборке и оценим ошибку на тестовой.

```{r}
# ядро генератора случайных чисел по номеру варианта
my.seed <- 4
set.seed(my.seed)

# обучающая выборка 50%
train <- sample(1:nrow(Boston), 200) #nrow(Boston)*0.5 - даёт слишком мало узлов

# тестовая выборка
medv.test <- Boston[-train,]
high.medv.test <- high.medv[-train]

# строим дерево на обучающей выборке
tree.medv <- tree(high.medv ~ .-medv, Boston, subset = train)
summary(tree.medv)
```

```{r}
# делаем прогноз
tree.pred <- predict(tree.medv, medv.test, type = "class")

# матрица неточностей
tbl <- table(tree.pred, high.medv.test)
tbl
```

```{r}
# ACC на тестовой
acc.test <- sum(diag(tbl))/sum(tbl)
names(acc.test)[length(acc.test)] <- 'medv.class.tree.all'
acc.test
```

Обобщённая характеристика точности: доля верных прогнозов: 0,856

# Бустинг (модель 2)

```{r}
set.seed(my.seed)
boost.medv <- gbm(high.medv ~ . -medv, data = Boston[train, ], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4)
# График и таблица относительной важности переменных
summary(boost.medv) 
```

```{r}
# прогноз
yhat.boost <- predict(boost.medv, newdata = Boston[-train, ], n.trees = 5000)

# MSE на тестовой
mse.test.2 <- mean((yhat.boost - medv.test)^2)
names(mse.test.2)[length(mse.test.2)] <- 'medv.boost.opt.model.2'
mse.test.2
```


```{r}
# График "прогноз - реализация"
plot(yhat.boost, Boston$high.medv[-train])
```

