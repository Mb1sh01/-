---
title: "Лабораторная 4"
author: "Блинов И И"
date: "21 03 2021"
output: html_document
---

```{r setup, include=FALSE} 

# загрузка пакетов 
library('ISLR') # загружаем пакет 
library('GGally') # графики совместного разброса переменных 
library('lmtest') # тесты остатков регрессионных моделей 
library('FNN') # алгоритм kNN 

knitr::opts_chunk$set(echo = TRUE) 

``` 


Цель: исследовать набор данных Auto {ISLR} с помощью линейной регрессионной модели. Задействовав все возможные регрессоры, сделать вывод о пригодности модели для прогноза. Сравнить с методом k ближайших соседей по MSE на тестовой выборке. 


```{r echo = F, warning = F, error = F} 

# константы 
my.seed <- 4
train.percent <- 0.85 

# загрузка данных 
#fileURL <- 'https://sites.google.com/a/kiber-guu.ru/msep/mag-econ.. 

# открываем данные 
data(Auto) 
#?Auto 

head(Auto) 


# преобразуем категориальные переменные в факторы 
#Auto <- read.csv(data(Auto), row.names = 1, sep = ';', as.is = T) 
Auto$cylinders <- as.factor(Auto$cylinders) 


Auto <- subset(Auto, select = c(mpg, weight, year, acceleration, cylinders)) 
str(Auto) 

# обучающая выборка 
set.seed(my.seed) 
inTrain <- sample(seq_along(Auto$mpg), 
nrow(Auto) * train.percent) 
df.train <- Auto[inTrain, c(colnames(Auto)[-1], colnames(Auto)[1])] 
df.test <- Auto[-inTrain, -1] 

``` 


####Описание переменных 
Набор данных wages содержит переменные: 

mpg - миль на галлон 
weight - Масса автомобиля (кг.) 
year - год модели машины (г.) 
acceleration - Время разгона от 0 до 60 миль в час (сек.) 
cylinders - Количество цилиндров от 4 до 8 
Размерность обучающей выборки: n = 397 строк, p = 4 объясняющих переменных. Зависимая переменная – mpg. 

##описательные статистики по переменным 
```{r echo = F, warning = F, error = F} 

summary(df.train) 

``` 


##совместный график разброса переменных 
```{r echo = F, warning = F, error = F} 

ggp <- ggpairs(df.train, upper = list(combo = 'box')) 
print(ggp, progress = F) 

```
```{r echo = F, warning = F, error = F}
ggp <- ggpairs(df.train[, c('cylinders', 'mpg')], 
               aes(color = cylinders), upper = list(combo = 'box'))
print(ggp, progress = F)
```
Коробчатые диаграммы на пересечении *mpg* и *cylinders* показывают, что самому большому количеству цилиндров  соответствует самый низкий расход миль на галлон. Самый высокий расход миль на галлон соответствует  4 цилинрам. Судя по графику в центре, наблюдения распределены по значениям переменой *cylinders* неравномерно: группы с 3 и 5 цилинрами самые немногочисленные.


#Модели:



```{r echo = F, warning = F, error = F}

model.1 <- lm(mpg ~ . + cylinders:acceleration + cylinders:year + cylinders:weight,
              data = df.train)
summary(model.1)
```

```{r echo = F, warning = F, error = F}



model.2 <- lm(mpg ~ . + cylinders:year +  cylinders:acceleration,
              data = df.train)
summary(model.2)
```

```{r echo = F, warning = F, error = F}

# Взаимодействие weight1:cylinders также исключаем.
model.3 <- lm(mpg ~ . + cylinders:year,
              data = df.train)
summary(model.3)

```

```{r echo = F, warning = F, error = F}

model.4 <- lm(mpg ~ weight + acceleration +year+ cylinders,
              data = df.train)
summary(model.4)
```
```{r echo = F, warning = F, error = F}

model.5 <- lm(mpg ~ weight + year+cylinders,
              data = df.train)
summary(model.5)
```
Модель значима!
```{r echo = F, warning = F, error = F}
df.train$cylinders <- as.numeric(df.train$cylinders)
df.test$cylinders <- as.numeric(df.test$cylinders)
```

```{r echo = F, warning = F, error = F}


model.6 <- lm(mpg ~ .,
              data = df.train)
summary(model.6)
```
```{r echo = F, warning = F, error = F}


model.7 <- lm(mpg ~ weight+year+ cylinders,
              data = df.train)
summary(model.7)
```
Наилучшая модель model.7

```{r echo = F, warning = F, error = F}

# тест Бройша-Пагана
bptest(model.7)
```

```{r echo = F, warning = F, error = F}

# статистика Дарбина-Уотсона
dwtest(model.7)
```
```{r echo = F, warning = F, error = F}

# графики остатков
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))
plot(model.7, 1)
plot(model.7, 4)
plot(model.7, 5)
```

В модели есть три влиятельных наблюдения: 323, 112, 387, – которые, однако, не выходят за пределы доверительных границ на третьем графике.

```{r echo = F, warning = F, error = F}

# фактические значения y на тестовой выборке
y.fact <- Auto[-inTrain, ]$mpg
y.model.lm <- predict(model.7, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)
```


```{r echo = F, warning = F, error = F}

# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(apply(df.train, 2, as.numeric))
df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))

for (i in 2:50){
    model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'mpg')], 
                     y = df.train.num[, 'mpg'], 
                     test = df.test.num, k = i)
    y.model.knn <- model.knn$pred
    if (i == 2){
        MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
    } else {
        MSE.knn <- c(MSE.knn, 
                     sum((y.model.knn - y.fact)^2) / length(y.model.knn))
    }
}

# график
par(mar = c(4.5, 4.5, 1, 1))
# ошибки kNN
plot(2:50, MSE.knn, type = 'b', col = 'darkgreen',
     xlab = 'значение k', ylab = 'MSE на тестовой выборке', ylim=c(11, 20))
# ошибка регрессии
lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)
legend('topright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))
```       
       При увеличении значения k мы получае наиболее точный результат.
       Как можно видеть по графику, ошибка регрессии на тестовой выборке больше, чем ошибка метода k ближайших соседей с k от 2 до 30. Далее с увеличением количества соседей точность kNN падает. Ошибка регрессионной модели на тестовой выборке очень велика и составляет 

$$\frac{\sqrt{MSE_{TEST}}}{\bar{y}_{TEST}} = 7.8%$$ 

от среднего значения зависимой переменной. Для модели регрессии это может означать присутствие важного объясняющего фактора.
       
       
